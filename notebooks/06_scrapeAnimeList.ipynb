{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6576d1a9-c7b7-44fa-9e3a-3c5f2ffffef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO \n",
    "import time\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcffb7f-c0cd-4616-a12d-eec102ac08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../tools/credentials.json') as file:\n",
    "    credentials = json.load(file)\n",
    "    \n",
    "username = credentials[\"dblogin\"][\"username\"]\n",
    "password = credentials[\"dblogin\"][\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b6a4ed-3c8a-4eec-a5c7-f36b25b78f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_string = f\"postgresql://{username}:{password}@192.168.0.3:5432/animeplanet\"\n",
    "db = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5d0c56-40fd-453c-b7d1-6815127c0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff2d96-0e42-41b8-9080-b7ac20d62851",
   "metadata": {},
   "source": [
    "### Get Anime List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1d902-3ec1-48fc-98a2-9dad3d4abd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('scraping anime list...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b272b19-78fd-4598-aea4-9da726bc8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.anime-planet.com/anime/top-anime?page='\n",
    "\n",
    "url = f'{base_url}{1}'\n",
    "resp = requests.get(f'http://192.168.0.3:5000/special-requests?url={quote(url)}')\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "ul = soup.find('ul', attrs={'class':'nav'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f6a78-5cf8-4b1d-8b51-3d516b89fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nums = []\n",
    "for tag in ul.find_all('a'):\n",
    "    try:\n",
    "        page_nums.append(int(tag.text))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "num_pages = max(page_nums)\n",
    "\n",
    "urls = [f'{base_url}{i}' for i in range(1, num_pages+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60669125-bfc7-4b43-ac97-f430b18811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeTable(url):\n",
    "    resp = requests.get(f'http://192.168.0.3:5000/special-requests?url={quote(url)}')\n",
    "    if resp.text != '':\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        chunk = pd.read_html(StringIO(str(table)), index_col='Rank')[0][['Title', 'Type', 'Year']]\n",
    "        chunk['url'] = [np.where(tag.has_attr('href'), \n",
    "                           'https://www.anime-planet.com' + tag.get('href'), \n",
    "                           'no link') for tag in table.find_all('a')]\n",
    "        chunk.columns = [col.lower() for col in chunk.columns]\n",
    "        chunk['url'] = chunk['url'].astype('string')\n",
    "        return chunk\n",
    "    else:\n",
    "        return scrapeTable(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9654f-4c50-422e-8519-6127aff23cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10\n",
    "df = pd.DataFrame()\n",
    "\n",
    "url_chunks = chunker(urls, chunksize)\n",
    "\n",
    "for idx, url_chunk in enumerate(tqdm(url_chunks, total=int(len(urls)/chunksize)+1), 1):\n",
    "    with ThreadPoolExecutor(max_workers=chunksize) as executor:\n",
    "        chunk = pd.concat(list(executor.map(scrapeTable, url_chunk)), ignore_index=True)\n",
    "        \n",
    "    df = pd.concat([df, chunk], ignore_index=True)\n",
    " \n",
    "    time.sleep(max(min(np.random.poisson(2), 5), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016263c-e1c0-4981-a29a-cabd2ad8a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['url'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07b4fb-c1d2-49e1-b9c9-7f877712c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saving data to file...')\n",
    "df.to_csv('../data/anime_list.csv.xz', index=False)\n",
    "\n",
    "with db.connect() as con:\n",
    "    print('removing from db...')\n",
    "    query = f\"\"\"DELETE FROM anime;\"\"\"\n",
    "    con.execute(sql.text(query))\n",
    "    \n",
    "    print('saving data to db...')\n",
    "    df.to_sql('anime', con, if_exists='append', index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59626afb-45a2-418e-bd16-4663d7b114fa",
   "metadata": {},
   "source": [
    "### Scrape Anime Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be649d16-86de-46b4-a555-6c2310730822",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('scraping anime pages...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfecbf-f385-4dc1-ae84-80a6f8d18727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('anime', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76680e11-4d46-4748-aafd-11118cab84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url, attempt=1):\n",
    "    if attempt == 4:\n",
    "        return (url, '')\n",
    "    resp = requests.get(f'http://192.168.0.3:5000/special-requests?url={quote(url)}')\n",
    "    return (url, resp.text) if resp.text != '' else getPage(url, attempt+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02192b75-8043-4308-abd2-d427c61431d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10\n",
    "\n",
    "url_list = df['url'].to_list()\n",
    "url_chunks = chunker(url_list, chunksize)\n",
    "\n",
    "url_html_dict = {}\n",
    "for url_chunk in tqdm(url_chunks, total=int(len(url_list)/chunksize)+1):\n",
    "    with ThreadPoolExecutor(max_workers=chunksize) as executor:\n",
    "        list_of_tup = list(executor.map(getPage, url_chunk))\n",
    "        for tup in list_of_tup:\n",
    "            url_html_dict[tup[0]] = tup[1]\n",
    "            \n",
    "    time.sleep(max(min(np.random.poisson(10), 30), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9f432-321a-4759-9918-a3873dd5560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['html_text'] = df['url'].map(url_html_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2a860-7473-4e0f-8b59-527dcf1241f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saving data to file...')\n",
    "df.to_csv('../data/anime_list_html.csv.xz', index=False)\n",
    "\n",
    "with db.connect() as con:\n",
    "    print('removing from db...')\n",
    "    query = f\"\"\"DELETE FROM web_scrape \n",
    "                WHERE url in ({str(df['url'].to_list())[1:-1]})\"\"\"\n",
    "    con.execute(sql.text(query))\n",
    "    print('saving data to db...')\n",
    "    chunks = chunker(df[['url', 'html_text']], 1000)\n",
    "    for chunk in tqdm(chunks):\n",
    "        chunk.to_sql('web_scrape', con, if_exists='append', index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadea1ee-dc7b-424c-9c5b-2ff01c222784",
   "metadata": {},
   "source": [
    "### Extracting addition info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34734c37-df02-4051-81bd-5b6d85f6e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/anime_list_html.csv.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818bdd7c-426c-4912-839e-7400efc5984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test.html', 'w') as file:\n",
    "    file.write(df.loc[df['title'] == 'Fullmetal Alchemist: Brotherhood', 'html_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c84db51-30ac-4a72-9ff5-c168834f906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseInfo(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    title = soup.find('h1', {'itemprop':'name'}).text\n",
    "\n",
    "    section = soup.find(attrs={'class': 'pure-g entryBar'})\n",
    "    num_eps = section.find('span', {'class':'type'})\n",
    "    if num_eps:\n",
    "        num_eps = num_eps.text.replace('\\n', ' ').strip()\n",
    "    else:\n",
    "        num_eps = None\n",
    "    \n",
    "    studio = section.find('a', {'href': re.compile(r'/anime/studios/.*')})\n",
    "    if studio:\n",
    "        studio = studio.text\n",
    "    else:\n",
    "        studio = None\n",
    "    \n",
    "    start_end_years = section.find('span', {'class': 'iconYear'})\n",
    "    if start_end_years:\n",
    "        start_end_years = start_end_years.text\n",
    "    else:\n",
    "        start_end_years = None\n",
    "    \n",
    "    season_year = section.find('a', {'href': re.compile(r'/anime/seasons/.*')})\n",
    "    if season_year:\n",
    "        season_year = season_year.text\n",
    "    else:\n",
    "        season_year = None\n",
    "        \n",
    "    rating = section.find('div', {'class': 'avgRating'}).text.replace('\\n', ' ').strip()\n",
    "    \n",
    "    tags_section = soup.find('div', {'class':'tags'})\n",
    "    if tags_section:\n",
    "        tags = tags_section.find_all('a', {'href': re.compile(r'/anime/tags/.*')})\n",
    "        tags = [tag.text.replace('\\n', ' ').strip() for tag in tags]\n",
    "    else:\n",
    "        tags = None\n",
    "    \n",
    "    cw_section = soup.find('div', {'class':'tags tags--plain'})\n",
    "    if cw_section:\n",
    "        content_warnings = [cw.text.replace('\\n', ' ').replace(',', '').strip() for cw in cw_section.find_all('li')]\n",
    "    else:\n",
    "        content_warnings = None\n",
    "        \n",
    "    synopsis = soup.find('p').text\n",
    "    url = soup.find('link', {'href': re.compile(r'https://www.anime-planet.com/anime/')})['href']\n",
    "    \n",
    "    return (title, num_eps, studio, start_end_years, season_year, rating, synopsis, tags, content_warnings, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae97575-93d9-429e-8734-00a80b31b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(14) as p:\n",
    "    list_of_tups = list(p.map(parseInfo, df['html_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb23429-d47b-4164-be65-79f9a046bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.DataFrame(list_of_tups, columns=['title', 'num_eps', 'studio', 'start_end_years', 'season_year', 'rating', \n",
    "                                            'synopsis', 'tags', 'content_warnings', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5fd7c6-15f7-4257-95cf-73864914c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.to_csv('../data/anime_raw.csv.xz', index=False)\n",
    "anime.to_pickle('../data/anime_raw.pkl.xz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
