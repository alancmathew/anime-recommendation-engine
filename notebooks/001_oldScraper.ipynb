{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/urls.txt\", \"r\") as f:\n",
    "    urls = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 418):\n",
    "    page_num = \"\" if i == 1 else f\"?page={i}\"\n",
    "    r = requests.get(\"https://www.anime-planet.com/anime/top-anime\" + page_num)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    for entry in soup.find_all(\"td\", attrs={\"class\": \"tableTitle\"}):\n",
    "        extract = BeautifulSoup(str(entry))\n",
    "        cur_url = \"https://www.anime-planet.com\" + extract.find(\"a\")[\"href\"]\n",
    "        if cur_url not in urls:\n",
    "            urls.append(cur_url)\n",
    "            with open(\"data/urls.txt\", \"a\") as f:\n",
    "                f.writeline(cur_url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0], \"https://www.anime-planet.com/anime/working-boys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(urls[0], timeout=1)\n",
    "soup = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"h1\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entryBar = BeautifulSoup(str(soup.find(\"section\", attrs={\"class\":\"entryBar\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### type, episodes, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entryBar.find(\"span\", attrs={\"class\":\"type\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### studios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[std.text for std in entryBar.find_all(\"a\", {\"href\" : lambda l: l and l.startswith(\"/anime/studios/\")})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### startYr, endYr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entryBar.find(\"span\", attrs={\"class\":\"iconYear\"}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entryBar.find(\"a\", {\"href\" : lambda l: l and l.startswith(\"/anime/seasons/\")}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rating, votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entryBar.find(\"div\", attrs={\"class\":\"avgRating\"})[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", attrs={\"class\":\"pure-1 md-3-5\"}).p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.text.strip() for s in BeautifulSoup(str(soup.find(\"div\", attrs={\"class\":\"tags\"}))).find_all(\"a\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.text.strip(\" \\n,\") for s in BeautifulSoup(str(soup.find(\"div\", attrs={\"class\":\"tags tags--plain\"}))).find_all(\"a\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSoup(i):\n",
    "    with webdriver.Chrome(options=chrome_options) as driver:\n",
    "        while True:\n",
    "            try:\n",
    "                driver.get(urls[i])\n",
    "                break\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "        \n",
    "        htmlSource = driver.page_source\n",
    "        \n",
    "    soup = BeautifulSoup(htmlSource)\n",
    "    del htmlSource\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator():\n",
    "    for i in range(12500, 14578, 100):\n",
    "        if i == 14500:\n",
    "            yield (i, 14578)\n",
    "        else:\n",
    "            yield (i, (i+100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [2:12:19, 378.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 10.6 s, total: 1min 55s\n",
      "Wall time: 2h 12min 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for start, stop in tqdm(iterator()):\n",
    "    \n",
    "    data_dict = {\"title\":[], \"mediaTypeEpsDur\":[], \"studios\":[], \"startEnd\":[], \"seasonWSF\":[],\n",
    "                 \"ratingVotes\":[], \"description\":[], \"tags\":[], \"contentWarn\":[], \n",
    "                 \"watched\":[], \"watching\":[], \"wantWatch\":[], \"dropped\":[]}\n",
    "    \n",
    "    for j in range(start, stop):\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                soup = getSoup(j)\n",
    "                title = soup.find(\"h1\").text\n",
    "                break\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        data_dict[\"title\"].append(title)\n",
    "\n",
    "        entryBar = BeautifulSoup(str(soup.find(\"section\", attrs={\"class\":\"entryBar\"})))\n",
    "\n",
    "        mediaTypeEpsDur = entryBar.find(\"span\", attrs={\"class\":\"type\"})\n",
    "        data_dict[\"mediaTypeEpsDur\"].append(mediaTypeEpsDur.text if mediaTypeEpsDur else np.NaN)\n",
    "\n",
    "        studios = [std.text for std in entryBar.find_all(\"a\", {\"href\" : lambda l: l and l.startswith(\"/anime/studios/\")})]\n",
    "        data_dict[\"studios\"].append(studios)\n",
    "\n",
    "        startEnd = entryBar.find(\"span\", attrs={\"class\":\"iconYear\"})\n",
    "        data_dict[\"startEnd\"].append(startEnd.text.strip() if startEnd else np.NaN)\n",
    "\n",
    "        seasonWSF = entryBar.find(\"a\", {\"href\" : lambda l: l and l.startswith(\"/anime/seasons/\")})\n",
    "        data_dict[\"seasonWSF\"].append(seasonWSF.text if seasonWSF else np.NaN)\n",
    "\n",
    "        ratingVotes = entryBar.find(\"div\", attrs={\"class\":\"avgRating\"})[\"title\"]\n",
    "        data_dict[\"ratingVotes\"].append(ratingVotes)\n",
    "\n",
    "        description = soup.find(\"div\", attrs={\"class\":\"pure-1 md-3-5\"}).p.text\n",
    "        data_dict[\"description\"].append(description)\n",
    "\n",
    "        tags = [s.text.strip() for s in BeautifulSoup(str(soup.find(\"div\", attrs={\"class\":\"tags\"}))).find_all(\"a\")]\n",
    "        data_dict[\"tags\"].append(tags)\n",
    "\n",
    "        contentWarn = [s.text.strip(\" \\n,\") \n",
    "                           for s in BeautifulSoup(str(soup.find(\"div\", attrs={\"class\":\"tags tags--plain\"}))).find_all(\"a\")]\n",
    "        data_dict[\"contentWarn\"].append(contentWarn)\n",
    "\n",
    "        userStats = [s.text for s in soup.find_all(\"span\", attrs={\"class\":\"slCount\"})]\n",
    "        if len(userStats) == 3:\n",
    "            data_dict[\"watched\"].append(np.NaN)\n",
    "            data_dict[\"watching\"].append(userStats[0])\n",
    "            data_dict[\"wantWatch\"].append(userStats[1])\n",
    "            data_dict[\"dropped\"].append(userStats[2])\n",
    "        else:\n",
    "            data_dict[\"watched\"].append(userStats[0])\n",
    "            data_dict[\"watching\"].append(userStats[1])\n",
    "            data_dict[\"wantWatch\"].append(userStats[2])\n",
    "            data_dict[\"dropped\"].append(userStats[3])\n",
    "\n",
    "        del soup\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = pd.read_csv(\"data/anime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([done] + df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"data/anime_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(i):\n",
    "    while True:\n",
    "        try:\n",
    "            soup = getSoup(i)\n",
    "            title = soup.find(\"h1\").text\n",
    "            break\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    entryBar = BeautifulSoup(str(soup.find(\"section\", attrs={\"class\":\"entryBar\"})))\n",
    "\n",
    "    mediaTypeEpsDur = entryBar.find(\"span\", attrs={\"class\":\"type\"})\n",
    "    mediaTypeEpsDur = mediaTypeEpsDur.text if mediaTypeEpsDur else np.NaN\n",
    "\n",
    "    studios = [std.text for std in entryBar.find_all(\"a\", {\"href\" : lambda l: l and l.startswith(\"/anime/studios/\")})]\n",
    "\n",
    "    startEnd = entryBar.find(\"span\", attrs={\"class\":\"iconYear\"})\n",
    "    startEnd = startEnd.text.strip() if startEnd else np.NaN\n",
    "\n",
    "    seasonWSF = entryBar.find(\"a\", {\"href\" : lambda l: l and l.startswith(\"/anime/seasons/\")})\n",
    "    seasonWSF = seasonWSF.text if seasonWSF else np.NaN\n",
    "\n",
    "    ratingVotes = entryBar.find(\"div\", attrs={\"class\":\"avgRating\"})[\"title\"]\n",
    "\n",
    "    description = soup.find(\"div\", attrs={\"class\":\"pure-1 md-3-5\"}).p.text\n",
    "\n",
    "    tags = [s.text.strip() for s in BeautifulSoup(str(soup.find(\"div\", attrs={\"class\":\"tags\"}))).find_all(\"a\")]\n",
    "\n",
    "    contentWarn = [s.text.strip(\" \\n,\") \n",
    "                       for s in BeautifulSoup(str(soup.find(\"div\", attrs={\"class\":\"tags tags--plain\"}))).find_all(\"a\")]\n",
    "\n",
    "    userStats = [s.text for s in soup.find_all(\"span\", attrs={\"class\":\"slCount\"})]\n",
    "    if len(userStats) == 3:\n",
    "        watching, wantWatch, dropped = userStats\n",
    "        watched = np.NaN\n",
    "    else:\n",
    "        watched, watching, wantWatch, dropped = userStats\n",
    "\n",
    "\n",
    "    return [title, mediaTypeEpsDur, studios, startEnd, seasonWSF, ratingVotes, description, \n",
    "            tags, contentWarn, watched, watching, wantWatch, dropped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "data_dict = {}\n",
    "\n",
    "p = Pool()\n",
    "\n",
    "data_dict[\"title\"], data_dict[\"mediaTypeEpsDur\"], data_dict[\"studios\"], data_dict[\"startEnd\"], data_dict[\"seasonWSF\"], \\\n",
    "data_dict[\"ratingVotes\"], data_dict[\"description\"], data_dict[\"tags\"], data_dict[\"contentWarn\"], data_dict[\"watched\"], \\\n",
    "data_dict[\"watching\"], data_dict[\"wantWatch\"], data_dict[\"dropped\"] = zip(*p.map(scraper, range(64)))\n",
    "    \n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
