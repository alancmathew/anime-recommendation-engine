{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "813ed3e6-0f62-496f-b513-d89a809a123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "from itertools import cycle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "sys.path.insert(0, '../tools/')\n",
    "from specialRequests import specialRequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34daa9b-b678-4842-8168-21faeabc27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimePlanetCrawler:\n",
    "    def __init__(self):\n",
    "        with open('../tools/credentials.json') as file:\n",
    "            credentials = json.load(file)\n",
    "\n",
    "        username = credentials[\"dblogin\"][\"username\"]\n",
    "        password = credentials[\"dblogin\"][\"password\"]\n",
    "\n",
    "        db_string = f\"postgresql://{username}:{password}@192.168.0.3:5432/animeplanet\"\n",
    "        self.db = create_engine(db_string)\n",
    "        \n",
    "        self.sr = specialRequests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60eb2058-4c84-4002-85bf-d3acc0e94bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(self):\n",
    "    print('loading data...')\n",
    "    self.urls = {}\n",
    "    with self.db.connect() as con:\n",
    "        self.urls['done'] = set(pd.read_sql('SELECT url FROM web_scrape WHERE html_text IS NOT NULL;', con)['url'].to_list())\n",
    "        self.urls['todo'] = set(pd.read_sql('SELECT url FROM web_scrape WHERE html_text IS NULL;', con)['url'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ed8a48-5930-405e-b98f-f26ccaad26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(self):\n",
    "    print('saving data...')\n",
    "    \n",
    "    with open('../data/urls.pkl','wb') as file:\n",
    "        pickle.dump(self.urls, file)\n",
    "        \n",
    "    with open(\"../data/urls.json\", 'w') as file:\n",
    "        json.dump({'done':list(self.urls['done']), 'todo':list(self.urls['todo'])}, file, indent=2) \n",
    "        \n",
    "    page_df = pd.DataFrame(self.page_data)\n",
    "    batch_df = page_df.loc[page_df['html_text'].notnull()][]\n",
    "    novel_df = page_df.loc[page_df['html_text'].isnull()]\n",
    "    \n",
    "    batch_urls = batch_df['url'].to_list()\n",
    "    novel_urls = novel_df['url'].to_list()\n",
    "    \n",
    "    df_inter = set(batch_urls).intersection(set(novel_urls))\n",
    "    dict_inter = self.urls['done'].intersection(self.urls['todo'])\n",
    "    \n",
    "    if (len(df_inter) != 0) or (len(dict_inter) != 0):\n",
    "        print('df_inter', df_inter)\n",
    "        print('dict_inter', dict_inter)\n",
    "    \n",
    "    with self.db.connect() as con:\n",
    "        print('\\tremoving popped todo data...')\n",
    "        con.execute(f\"DELETE FROM web_scrape WHERE url in ({str(batch_urls)[1:-1]})\")\n",
    "        \n",
    "        print('\\tsaving done data...')\n",
    "        batch_df.to_sql('web_scrape', con, index=False, if_exists='append')\n",
    "\n",
    "        try:\n",
    "            print('\\tsaving todo data...')\n",
    "            novel_df.to_sql('web_scrape', con, index=False, if_exists='append')\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            con.execute(f\"UPDATE web_scrape SET html_text = NULL WHERE url IN ({str(done_urls)[1:-1]})\")\n",
    "            self.urls['done'] = self.urls['done'].difference(batch_urls)\n",
    "            self.urls['todo'] = self.urls['todo'].difference(novel_urls)\n",
    "            self.urls['todo'] = self.urls['todo'].union(batch_urls)\n",
    "        \n",
    "    del page_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d877e1c8-125a-4438-be5a-f7520a313dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapePage(self, url):    \n",
    " \n",
    "    cur_urls = set()\n",
    "\n",
    "    html_text = self.sr.get(url)\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        try:\n",
    "            branch = link.get('href')\n",
    "            if branch[0] == '/':\n",
    "                cur_urls.add('https://www.anime-planet.com' + branch)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return cur_urls, (url, html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51831b5-0de5-449a-bb03-3214bb96162b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def popTodoSet(self):\n",
    "    disallowed_urls = ['https://www.anime-planet.com/search.php', 'https://www.anime-planet.com/login', \n",
    "                       'https://www.anime-planet.com/sign-up']\n",
    "    \n",
    "    dist_to25 = (25 - (len(self.urls['done']) % 25))\n",
    "    \n",
    "    popped_urls = set()\n",
    "    while len(popped_urls) < dist_to25:\n",
    "        pop_url = self.urls['todo'].pop()\n",
    "\n",
    "        if pop_url[-1] == '.':\n",
    "            old_url = pop_url\n",
    "            pop_url = pop_url.replace('forum/members', 'users')[:-1]\n",
    "            if (old_url not in self.urls['done']) and (old_url not in disallowed_urls):\n",
    "                self.urls['done'].add(old_url)\n",
    "                self.page_data['url'].append(old_url)\n",
    "                self.page_data['html_text'].append('failed scrape')\n",
    "                \n",
    "        if (pop_url not in self.urls['done']) and (pop_url not in disallowed_urls):\n",
    "            popped_urls.add(pop_url)\n",
    "            \n",
    "    return popped_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99266e49-89f7-4e42-a88c-70b3c7181165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCrawlResults(self, results):\n",
    "    cur_urls = set().union(*map(lambda x: x[0], results))\n",
    "    url_html_tup = map(lambda x: x[1], results)\n",
    "    for url, html_text in url_html_tup:\n",
    "        self.urls['done'].add(url)\n",
    "        self.page_data['url'].append(url)\n",
    "        if html_text == '':\n",
    "            self.page_data['html_text'].append('failed scrape')\n",
    "        else:\n",
    "            self.page_data['html_text'].append(html_text)\n",
    "      \n",
    "    novel = (cur_urls.difference(self.urls['todo'])).difference(self.urls['done'])\n",
    "            \n",
    "    self.urls['todo'] = self.urls['todo'].union(novel)\n",
    "\n",
    "    for url in novel:\n",
    "        self.page_data['url'].append(url)\n",
    "        self.page_data['html_text'].append(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de0e86c-5e96-47c3-a6af-72e2f661fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCrawlProgress(self):\n",
    "    len_done = len(self.urls['done'])\n",
    "    len_todo = len(self.urls['todo'])\n",
    "    print(len_todo, len_done, 0 if len_todo == 0 else len_done/(len_todo+len_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0273f1-2f12-4e9e-9a08-6893d895b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(self):\n",
    "    self.loadData()\n",
    "    print('starting crawl...')\n",
    "    self.page_data = {'url':[], 'html_text':[]}\n",
    "    start_time = time.time()\n",
    "    while len(self.urls['todo']) > 0:\n",
    "        \n",
    "        popped_urls = self.popTodoSet()    \n",
    "            \n",
    "        with ThreadPoolExecutor(max_workers=25) as executor:\n",
    "            results = list(executor.map(self.scrapePage, list(popped_urls)))\n",
    "        \n",
    "        self.processCrawlResults(results)\n",
    "        \n",
    "        self.printCrawlProgress()\n",
    "        \n",
    "        len_done = len(self.urls['done'])\n",
    "        if len_done % 500 == 0:\n",
    "            end_time = time.time()\n",
    "            print('timer: ', end_time-start_time)\n",
    "            self.saveData()\n",
    "            self.loadData()\n",
    "            self.page_data = {'url':[], 'html_text':[]}\n",
    "            print('starting crawl...')\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55f7c8c1-0b79-4742-9d82-4c049b41a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnimePlanetCrawler.loadData = loadData\n",
    "AnimePlanetCrawler.saveData = saveData\n",
    "AnimePlanetCrawler.scrapePage = scrapePage\n",
    "AnimePlanetCrawler.popTodoSet = popTodoSet\n",
    "AnimePlanetCrawler.processCrawlResults = processCrawlResults\n",
    "AnimePlanetCrawler.printCrawlProgress = printCrawlProgress\n",
    "AnimePlanetCrawler.crawl = crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398b7507-ad64-4fe8-8416-368353597fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = AnimePlanetCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e15221-f435-47bc-9867-a0a7cd89b6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "starting crawl...\n",
      "1572932 576825 0.2683210241901759\n",
      "1572965 576850 0.26832541404725524\n",
      "1572992 576875 0.2683305525411572\n",
      "1573032 576901 0.2683344085606389\n",
      "1573034 576925 0.26834232652808726\n",
      "1573064 576950 0.2683470898329034\n",
      "1573093 576975 0.2683519777048912\n",
      "1573157 577000 0.2683524970502154\n",
      "timer:  48.73614525794983\n",
      "saving data...\n",
      "\tremoving popped todo data...\n",
      "\tsaving done data...\n",
      "\tsaving todo data...\n",
      "(psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"web_scrape_pk\"\n",
      "DETAIL:  Key (url)=(https://www.anime-planet.com/users/Pandakilla/manga/read) already exists.\n",
      "\n",
      "[SQL: INSERT INTO web_scrape (url, html_text) VALUES (%(url)s, %(html_text)s)]\n",
      "[parameters: ({'url': 'https://www.anime-planet.com/characters/machiko-kano/hates', 'html_text': None}, {'url': 'https://www.anime-planet.com/characters/penguin-2/comments', 'html_text': None}, {'url': 'https://www.anime-planet.com/characters/narrator-gosenzo-sama-banbanzai/lists', 'html_text': None}, {'url': 'https://www.anime-planet.com/users/Prathameshv17/feed?type=lists', 'html_text': None}, {'url': 'https://www.anime-planet.com/users/TumiJumi/comments', 'html_text': None}, {'url': 'https://www.anime-planet.com/users/Prathameshv17/feed?type=favs', 'html_text': None}, {'url': 'https://www.anime-planet.com/characters/machiko-kano/loves', 'html_text': None}, {'url': 'https://www.anime-planet.com/characters/penguin-2/loves', 'html_text': None}  ... displaying 10 of 423 total bound parameter sets ...  {'url': 'https://www.anime-planet.com/users/Jucymango1/lists?filter=people', 'html_text': None}, {'url': 'https://www.anime-planet.com/people/rats/lists', 'html_text': None})]\n",
      "(Background on this error at: https://sqlalche.me/e/14/gkpj)\n",
      "loading data...\n",
      "starting crawl...\n",
      "1572933 576825 0.2683208993756507\n",
      "1572966 576850 0.26832528923405535\n",
      "1573011 576875 0.26832818112216184\n",
      "1573045 576901 0.26833278603276545\n",
      "1573048 576925 0.26834057916076154\n",
      "1573063 576950 0.26834721464474864\n",
      "1573090 576975 0.2683523521381912\n",
      "1573146 577000 0.2683538699232517\n",
      "timer:  49.89805793762207\n",
      "saving data...\n",
      "\tremoving popped todo data...\n",
      "\tsaving done data...\n",
      "\tsaving todo data...\n",
      "loading data...\n",
      "starting crawl...\n",
      "1573178 577025 0.2683583829061721\n",
      "1573243 577050 0.268358777152695\n",
      "1573351 577075 0.2683538052460303\n",
      "1573397 577100 0.2683565705974014\n",
      "1573421 577125 0.2683620810715046\n",
      "1573455 577150 0.2683663434242922\n",
      "1573517 577175 0.2683671116087287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-26686de0f395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1acb058b06e1>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrapePage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopped_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessCrawlResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintCrawlProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-70495e9da84e>\u001b[0m in \u001b[0;36mprocessCrawlResults\u001b[0;34m(self, results)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnovel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcur_urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'todo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'todo'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'todo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnovel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnovel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3b4ff-ac42-4ef4-b4f2-7952b3cbaca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
