{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6576d1a9-c7b7-44fa-9e3a-3c5f2ffffef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO \n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcffb7f-c0cd-4616-a12d-eec102ac08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../tools/credentials.json') as file:\n",
    "    credentials = json.load(file)\n",
    "    \n",
    "username = credentials[\"dblogin\"][\"username\"]\n",
    "password = credentials[\"dblogin\"][\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b6a4ed-3c8a-4eec-a5c7-f36b25b78f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_string = f\"postgresql://{username}:{password}@localhost:5432/animeplanet\"\n",
    "db = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5d0c56-40fd-453c-b7d1-6815127c0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff2d96-0e42-41b8-9080-b7ac20d62851",
   "metadata": {},
   "source": [
    "### Get Anime List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1d902-3ec1-48fc-98a2-9dad3d4abd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('scraping anime list...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b272b19-78fd-4598-aea4-9da726bc8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.anime-planet.com/anime/top-anime?page='\n",
    "\n",
    "url = f'{base_url}{1}'\n",
    "resp = requests.get(f'http://192.168.0.3:5000/special-requests?url={quote(url)}')\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "ul = soup.find('ul', attrs={'class':'nav'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f6a78-5cf8-4b1d-8b51-3d516b89fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nums = []\n",
    "for tag in ul.find_all('a'):\n",
    "    try:\n",
    "        page_nums.append(int(tag.text))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "num_pages = max(page_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60669125-bfc7-4b43-ac97-f430b18811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeTable(url):\n",
    "    resp = requests.get(f'http://192.168.0.3:5000/special-requests?url={quote(url)}')\n",
    "    if resp.text != '':\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        chunk = pd.read_html(StringIO(str(table)), index_col='Rank')[0][['Title', 'Type', 'Year']]\n",
    "        chunk['Url'] = [np.where(tag.has_attr('href'), \n",
    "                           'https://www.anime-planet.com' + tag.get('href'), \n",
    "                           'no link') for tag in table.find_all('a')]\n",
    "        return chunk\n",
    "    else:\n",
    "        return scrapeTable(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964b77d-1f14-4c27-9ee7-9ae7dcdae570",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [f'{base_url}{i}' for i in range(1, num_pages+1)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    df_list = list(executor.map(scrapeTable, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4ba42-cb15-4696-81e2-0b1c55cbae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53400b18-9bb8-410f-9490-45abc21d59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col.lower() for col in df.columns]\n",
    "df['url'] = df['url'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016263c-e1c0-4981-a29a-cabd2ad8a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[~df.duplicated(['url'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07b4fb-c1d2-49e1-b9c9-7f877712c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saving data to file...')\n",
    "df.to_csv('../data/anime_list.csv', index=False)\n",
    "\n",
    "with db.connect() as con:\n",
    "    print('removing from db...')\n",
    "    query = f\"\"\"DELETE FROM anime;\"\"\"\n",
    "    con.execute(sql.text(query))\n",
    "    \n",
    "    print('saving data to db...')\n",
    "    chunks = chunker(df, 1000)\n",
    "    for chunk in tqdm(chunks):\n",
    "        chunk.to_sql('anime', con, if_exists='append', index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59626afb-45a2-418e-bd16-4663d7b114fa",
   "metadata": {},
   "source": [
    "### Scrape Anime Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be649d16-86de-46b4-a555-6c2310730822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping anime pages...\n"
     ]
    }
   ],
   "source": [
    "print('scraping anime pages...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53cfecbf-f385-4dc1-ae84-80a6f8d18727",
   "metadata": {},
   "outputs": [],
   "source": [
    "with db.connect() as con:\n",
    "    df = pd.read_sql('anime', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76680e11-4d46-4748-aafd-11118cab84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url, attempt=1):\n",
    "    if attempt == 4:\n",
    "        return (url, '')\n",
    "    resp = requests.get(f'http://192.168.0.3:5000/special-requests?url={quote(url)}')\n",
    "    return (url, resp.text) if resp.text != '' else getPage(url, attempt+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02192b75-8043-4308-abd2-d427c61431d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343it [1:01:07, 10.69s/it]                            \n"
     ]
    }
   ],
   "source": [
    "chunksize = 50\n",
    "\n",
    "url_list = df['url'].to_list()\n",
    "url_chunks = chunker(url_list, chunksize)\n",
    "\n",
    "url_html_dict = {}\n",
    "for url_chunk in tqdm(url_chunks, total=len(url_list)/chunksize):\n",
    "    with ThreadPoolExecutor(max_workers=25) as executor:\n",
    "        list_of_tup = list(executor.map(getPage, url_chunk))\n",
    "        for tup in list_of_tup:\n",
    "            url_html_dict[tup[0]] = tup[1]\n",
    "    time.sleep(random.randint(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef9f432-321a-4759-9918-a3873dd5560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['html_text'] = df['url'].map(url_html_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f2a860-7473-4e0f-8b59-527dcf1241f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data to file...\n",
      "removing from db...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data to db...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [01:13,  4.09s/it]\n"
     ]
    }
   ],
   "source": [
    "print('saving data to file...')\n",
    "df.to_csv('../data/anime_list_html.csv.xz', index=False)\n",
    "\n",
    "with db.connect() as con:\n",
    "    print('removing from db...')\n",
    "    query = f\"\"\"DELETE FROM web_scrape \n",
    "                WHERE url in ({str(df['url'].to_list())[1:-1]})\"\"\"\n",
    "    con.execute(sql.text(query))\n",
    "    print('saving data to db...')\n",
    "    chunks = chunker(df[['url', 'html_text']], 1000)\n",
    "    for chunk in tqdm(chunks):\n",
    "        chunk.to_sql('web_scrape', con, if_exists='append', index=False, method='multi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
