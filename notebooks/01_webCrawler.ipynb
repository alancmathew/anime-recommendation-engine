{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43de628-2c7c-4dc3-b5f5-ed0204b5b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "from itertools import cycle\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "sys.path.insert(0, '../tools/')\n",
    "from specialRequests import specialRequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d4671-3b6b-4658-a3e2-9144799c58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../tools/credentials.json') as file:\n",
    "    credentials = json.load(file)\n",
    "    \n",
    "username = credentials[\"dblogin\"][\"username\"]\n",
    "password = credentials[\"dblogin\"][\"password\"]\n",
    "\n",
    "db_string = f\"postgresql://{username}:{password}@192.168.0.3:5432/animeplanet\"\n",
    "db = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba925cb-20b9-43f2-a1ae-5193900014a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    print('loading data...')\n",
    "    urls = {}\n",
    "    with db.connect() as con:\n",
    "        urls['done'] = set(pd.read_sql('SELECT url FROM web_scrape WHERE html_text IS NOT NULL;', con)['url'].to_list())\n",
    "        urls['todo'] = set(pd.read_sql('SELECT url FROM web_scrape WHERE html_text IS NULL;', con)['url'].to_list())\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e25ad-063a-4bd8-a9ab-e23ec182d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(page_data, urls):\n",
    "    print('saving data...')\n",
    "    \n",
    "    with open('../data/urls.pkl','wb') as file:\n",
    "        pickle.dump(urls, file)\n",
    "        \n",
    "    with open(\"../data/urls.json\", 'w') as file:\n",
    "        json.dump({'done':list(urls['done']), 'todo':list(urls['todo'])}, file, indent=2) \n",
    "        \n",
    "    page_df = pd.DataFrame(page_data)\n",
    "    done_df = page_df.loc[page_df['html_text'].notnull()]\n",
    "    todo_df = page_df.loc[page_df['html_text'].isnull()]\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    with db.connect() as con:\n",
    "        print('\\tremoving popped todo data...')\n",
    "        con.execute(f\"DELETE FROM web_scrape WHERE url in ({str(done_df['url'].to_list())[1:-1]})\")\n",
    "        \n",
    "        print('\\tsaving done data...')\n",
    "        done_df.to_sql('web_scrape', con, index=False, if_exists='append')\n",
    "#         for i in tqdm(range(done_df.shape[0])):\n",
    "#             try:\n",
    "#                 done_df.iloc[i:i+1].to_sql('web_scrape', con, index=False, if_exists='append')\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "        print('\\tsaving todo data...')\n",
    "        todo_df.to_sql('web_scrape', con, index=False, if_exists='append')\n",
    "#         for i in tqdm(range(todo_df.shape[0])):\n",
    "#             try:\n",
    "#                 todo_df.iloc[i:i+1].to_sql('web_scrape_todo', con, index=False, if_exists='append')\n",
    "#             except:\n",
    "#                 pass\n",
    "        \n",
    "    del page_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa2f55-0bdf-44b3-a420-838f2098cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = specialRequests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c968e-a798-46a7-a3e9-7cc635a6bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):    \n",
    "    \n",
    "    time.sleep(random.randint(200, 2000)/1000)\n",
    "    \n",
    "    cur_urls = set()\n",
    "\n",
    "    html_text = sr.get(url)\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        try:\n",
    "            branch = link.get('href')\n",
    "            if branch[0] == '/':\n",
    "                cur_urls.add('https://www.anime-planet.com' + branch)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return cur_urls, (url, html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c776f3b-66c4-465e-8cc3-da41aaa0cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getAllUrls():\n",
    "#     page_data = {'url':[], 'html_text':[]}\n",
    "#     disallowed_urls = ['https://www.anime-planet.com/search.php', 'https://www.anime-planet.com/login',\n",
    "#                        'https://www.anime-planet.com/sign-up']\n",
    "#     while len(urls['novel']) > 0:\n",
    "#         pop_url = urls['novel'].pop()\n",
    "\n",
    "#         if pop_url[-1] == '.':\n",
    "#             pop_url = pop_url.replace('forum/members', 'users')[:-1]\n",
    "\n",
    "#         if (pop_url not in urls['done']) and (pop_url not in disallowed_urls):\n",
    "#             cur_urls, html_text = getCurrentPageUrls(pop_url)\n",
    "#             urls['done'].add(pop_url)\n",
    "#             page_data['url'].append(pop_url)\n",
    "#             page_data['html_text'].append(html_text)\n",
    "        \n",
    "#             diff = cur_urls.difference(urls['done'])\n",
    "#             urls['novel'] = urls['novel'].union(diff)\n",
    "        \n",
    "#             print(len(urls['novel']), len(urls['done']), 0 if len(urls['novel']) == 0 else len(urls['done'])/len(urls['novel']), pop_url)\n",
    "        \n",
    "#             if len(urls['done']) % 100 == 0:\n",
    "#                 print('saving data...')\n",
    "#                 saveData(page_data)\n",
    "#                 page_data = {'url':[], 'html_text':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8dc80-123f-4818-8f0a-087019500d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllPagesMulti():\n",
    "    urls = loadData()\n",
    "    print('starting crawl...')\n",
    "    page_data = {'url':[], 'html_text':[]}\n",
    "    disallowed_urls = ['https://www.anime-planet.com/search.php', 'https://www.anime-planet.com/login',\n",
    "                       'https://www.anime-planet.com/sign-up']\n",
    "    start_time = time.time()\n",
    "    while len(urls['todo']) > 0:\n",
    "        dist_to25 = (25 - (len(urls['done']) % 25))\n",
    "        \n",
    "        popped_urls = []\n",
    "        while len(popped_urls) < dist_to25:\n",
    "            pop_url = urls['todo'].pop()\n",
    "            \n",
    "            if pop_url[-1] == '.':\n",
    "                old_url = pop_url\n",
    "                pop_url = pop_url.replace('forum/members', 'users')[:-1]\n",
    "                urls['done'].add(old_url)\n",
    "                page_data['url'].append(old_url)\n",
    "                page_data['html_text'].append('failed scrape')\n",
    "\n",
    "            if (pop_url not in urls['done']) and (pop_url not in disallowed_urls):\n",
    "                popped_urls.append(pop_url)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=25) as executor:\n",
    "            results = list(executor.map(getPage, popped_urls))\n",
    "\n",
    "        urls['done'] = urls['done'].union(set(popped_urls))\n",
    "\n",
    "        cur_urls = set().union(*[item[0] for item in results])\n",
    "        list_of_tuples = [item[1] for item in results]\n",
    "        for tup in list_of_tuples:\n",
    "            page_data['url'].append(tup[0])\n",
    "            if tup[1] == '':\n",
    "                page_data['html_text'].append('failed scrape')\n",
    "            else:\n",
    "                page_data['html_text'].append(tup[1])\n",
    "\n",
    "        novel = (cur_urls.difference(urls['done'])).difference(urls['todo'])\n",
    "        urls['todo'] = urls['todo'].union(novel)\n",
    "        \n",
    "        for link in novel:\n",
    "            page_data['url'].append(link)\n",
    "            page_data['html_text'].append(np.NaN)\n",
    "        \n",
    "        print(len(urls['todo']), len(urls['done']), 0 if len(urls['todo']) == 0 else len(urls['done'])/(len(urls['todo'])+len(urls['done'])))\n",
    "\n",
    "        len_done = len(urls['done'])\n",
    "        if len_done % 500 == 0:\n",
    "            end_time = time.time()\n",
    "            print('timer: ', end_time-start_time)\n",
    "            saveData(page_data, urls)\n",
    "            page_data = {'url':[], 'html_text':[]}\n",
    "            print('starting crawl...')\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe2caf-bf42-449b-bbfb-97ab8dd39701",
   "metadata": {},
   "outputs": [],
   "source": [
    "getAllPagesMulti()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
