{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ed3e6-0f62-496f-b513-d89a809a123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "from itertools import cycle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote\n",
    "\n",
    "# sys.path.insert(0, '../tools/')\n",
    "# from specialRequests import specialRequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34daa9b-b678-4842-8168-21faeabc27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimePlanetCrawler:\n",
    "    def __init__(self):\n",
    "        with open('../tools/credentials.json') as file:\n",
    "            credentials = json.load(file)\n",
    "\n",
    "        username = credentials[\"dblogin\"][\"username\"]\n",
    "        password = credentials[\"dblogin\"][\"password\"]\n",
    "\n",
    "        db_string = f\"postgresql://{username}:{password}@localhost:5432/animeplanet\"\n",
    "        self.db = create_engine(db_string)\n",
    "        \n",
    "#         self.sr = specialRequests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb2058-4c84-4002-85bf-d3acc0e94bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(self):\n",
    "    print('loading data...')\n",
    "    with self.db.connect() as con:\n",
    "        query = \"\"\"SELECT url \n",
    "                    FROM web_scrape \n",
    "                    WHERE html_text IS NOT NULL;\"\"\"\n",
    "        self.done = set(pd.read_sql(sqlalchemy.text(query), con)['url'].to_list())\n",
    "        \n",
    "        query = \"\"\"SELECT url \n",
    "                    FROM web_scrape \n",
    "                    WHERE html_text IS NULL;\"\"\"\n",
    "        self.pending = set(pd.read_sql(sqlalchemy.text(query), con)['url'].to_list())\n",
    "        \n",
    "        self.novel = set()\n",
    "        self.batch = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed8a48-5930-405e-b98f-f26ccaad26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(self):\n",
    "    print('saving data...')\n",
    "    \n",
    "    self.novel = self.novel.difference(set(self.batch.keys()))\n",
    "    self.pending = self.pending.difference(set(self.batch.keys()))\n",
    "    \n",
    "    data_dict = {'done': list(self.done), \n",
    "                 'pending': list(self.pending),\n",
    "                 'novel': list(self.novel),\n",
    "                 'batch': self.batch}\n",
    "    \n",
    "    with open('../data/urls.pkl','wb') as file:\n",
    "        pickle.dump(data_dict, file)\n",
    "    \n",
    "    with open(\"../data/urls.json\", 'w') as file:\n",
    "        json.dump(data_dict, file, indent=2) \n",
    "        \n",
    "    batch_dict = {'url': list(self.batch.keys()),\n",
    "                  'html_text': list(self.batch.values())}\n",
    "    batch_df = pd.DataFrame(batch_dict)\n",
    "    \n",
    "    novel_dict = {'url': list(self.novel),\n",
    "                  'html_text': [np.NaN for _ in range(len(self.novel))]}\n",
    "    novel_df = pd.DataFrame(novel_dict)\n",
    "    \n",
    "    batch_urls = batch_dict['url']\n",
    "    novel_urls = novel_dict['url']\n",
    "    \n",
    "    with self.db.connect() as con:\n",
    "        print('\\tremoving popped pending data...')\n",
    "        query = f\"\"\"DELETE FROM web_scrape \n",
    "                    WHERE url in ({str(batch_urls)[1:-1]})\"\"\"\n",
    "        con.execute(sqlalchemy.text(query))\n",
    "        \n",
    "        print('\\tsaving done data...')\n",
    "        batch_df.to_sql('web_scrape', con, index=False, if_exists='append', method='multi')\n",
    "\n",
    "        try:\n",
    "            print('\\tsaving pending data...')\n",
    "            novel_df.to_sql('web_scrape', con, index=False, if_exists='append', method='multi')\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            query = f\"\"\"UPDATE web_scrape \n",
    "                        SET html_text = NULL \n",
    "                        WHERE url IN ({str(batch_urls)[1:-1]})\"\"\"\n",
    "            con.execute(sqlalchemy.text(query))\n",
    "            self.done = self.done.difference(batch_urls)\n",
    "            self.pending = self.pending.difference(novel_urls)\n",
    "            self.pending = self.pending.union(batch_urls)\n",
    "        \n",
    "    self.batch = {}\n",
    "    self.novel = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51831b5-0de5-449a-bb03-3214bb96162b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def popBatch(self):\n",
    "    \n",
    "    dist_to10 = 10 - (len(self.done) % 10)\n",
    "    \n",
    "    popped_urls = set()\n",
    "    while len(popped_urls) < dist_to10:\n",
    "        pop_url = self.pending.pop()\n",
    "\n",
    "        if pop_url[-1] == '.':\n",
    "            new_url = pop_url.replace('forum/members', 'users')[:-1]\n",
    "            self.pending.add(new_url)\n",
    "                \n",
    "        popped_urls.add(pop_url)\n",
    "            \n",
    "    return popped_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877e1c8-125a-4438-be5a-f7520a313dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapePage(url):\n",
    "    \n",
    "    if ('forum/members' in url) and (url[-1] == '.'):\n",
    "        return (url, '')\n",
    "\n",
    "    resp = requests.get(f'http://192.168.0.3:5000/special-requests?url={quote(url)}')\n",
    "    html_text = resp.text\n",
    "#     html_text = self.sr.get(url)\n",
    "    \n",
    "    return (url, html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3537a6e-39d2-464c-a25d-d1a056789bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapePages(urls):\n",
    "    \n",
    "    resp = requests.post(f'http://192.168.0.3:5000/special-requests', json={'url':urls})\n",
    "    url_html_dict = resp.json()\n",
    "#     html_text = self.sr.get(url)\n",
    "    \n",
    "    return url_html_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cca03d-5201-44ee-8e6c-293346b31e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePage(html_text):\n",
    "    if html_text == '':\n",
    "        return set()\n",
    "\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    links = [str(a.get('href')) for a in soup.find_all('a')]\n",
    "    in_domain_links = filter(lambda x: x and x[0] == '/', links)\n",
    "    cur_urls = set([f'https://www.anime-planet.com{link}' for link in in_domain_links])\n",
    "    \n",
    "    return cur_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99266e49-89f7-4e42-a88c-70b3c7181165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCrawlResults(self, url_html_tup):\n",
    "    html_text_list = [x[1] for x in url_html_tup]\n",
    "    with Pool(4) as p:\n",
    "        cur_urls_set_list = p.map(parsePage, html_text_list)\n",
    "    cur_urls = set().union(*cur_urls_set_list)\n",
    "    for url, html_text in url_html_tup:\n",
    "        self.done.add(url)\n",
    "        self.batch[url] = 'failed scrape' if html_text == '' else html_text\n",
    "    \n",
    "    cur_urls = (cur_urls.difference(self.pending)).difference(self.done)\n",
    "    self.novel.update(cur_urls)\n",
    "    self.pending.update(cur_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0e86c-5e96-47c3-a6af-72e2f661fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCrawlProgress(self):\n",
    "    len_done = len(self.done)\n",
    "    len_pending = len(self.pending)\n",
    "    print(len_pending, len_done, 0 if len_pending == 0 else len_done/(len_pending+len_done))\n",
    "    return len_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee83e2-8e33-4796-9424-9a69fdd7831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waiter(secs):\n",
    "    print(f'waiting {secs} secs...')\n",
    "    for _ in tqdm(range(secs)):\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0273f1-2f12-4e9e-9a08-6893d895b999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crawl(self):\n",
    "    self.loadData()\n",
    "    print('starting crawl...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while len(self.pending) > 0:\n",
    "\n",
    "        popped_urls = self.popBatch()    \n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            url_html_tup = list(executor.map(scrapePage, popped_urls))\n",
    "        \n",
    "        self.processCrawlResults(url_html_tup)\n",
    "\n",
    "        len_done = self.printCrawlProgress()\n",
    "\n",
    "        \n",
    "        if len_done % 100 == 0:\n",
    "            end_time = time.time()\n",
    "            print('timer: ', end_time-start_time)\n",
    "            if len_done % 1000 == 0:\n",
    "                self.saveData()\n",
    "                if len_done % 500000 == 0:\n",
    "                    sleep_time = random.randint(3600*3, 3600*5)\n",
    "                    waiter(sleep_time)\n",
    "                elif len_done % 100000 == 0:\n",
    "                    sleep_time = random.randint(1800, 3600)\n",
    "                    waiter(sleep_time)\n",
    "                elif len_done % 10000 == 0:\n",
    "                    sleep_time = random.randint(300, 600)\n",
    "                    waiter(sleep_time)\n",
    "                print('starting crawl...')\n",
    "                \n",
    "            else:\n",
    "                time.sleep(random.randint(5, 10))\n",
    "                \n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7c8c1-0b79-4742-9d82-4c049b41a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnimePlanetCrawler.loadData = loadData\n",
    "AnimePlanetCrawler.saveData = saveData\n",
    "AnimePlanetCrawler.popBatch = popBatch\n",
    "AnimePlanetCrawler.processCrawlResults = processCrawlResults\n",
    "AnimePlanetCrawler.printCrawlProgress = printCrawlProgress\n",
    "AnimePlanetCrawler.crawl = crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b7507-ad64-4fe8-8416-368353597fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crawler = AnimePlanetCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e15221-f435-47bc-9867-a0a7cd89b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476981b4-77aa-4f5d-bc6a-8d20c24102f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
